# BielikDeepEvalTestsExample ‚Äî Evals (PL / EN)

Repozytorium zawiera lokalne testy ewaluacyjne (rules/golden/judge) dla modeli LLM uruchamianych przez **Ollama** i ocenianych przez **DeepEval**.

This repository contains local evaluation tests (rules/golden/judge) for LLMs running via **Ollama** and evaluated with **DeepEval**.


- [PL üáµüá± ‚Äî Ewaluacje (rules / golden / judge)](#pl--ewaluacje-rules--golden--judge)
- [EN üá¨üáß ‚Äî Evals (rules / golden / judge)](#en--evals-rules--golden--judge)

---

# PL üáµüá± ‚Äî Ewaluacje (rules / golden / judge)

Ten katalog zawiera zestaw test√≥w ewaluacyjnych dla modelu jƒôzykowego (LLM), przygotowany tak, aby:

- by≈Ç **≈Çatwy do uruchomienia lokalnie** (CPU-only te≈º dzia≈Ça),
- mia≈Ç **czytelne raporty HTML** (prompt + output modelu),
- wspiera≈Ç **CI/CD** (JSON report),
- da≈Ço siƒô nad nim pracowaƒá zespo≈Çowo (cases w `.jsonl`, ≈Çatwe PR-y).

---

## Jak to dzia≈Ça?

Wspieramy 3 typy test√≥w:

### 1) **Rules tests** (`rules.jsonl`)
Szybkie, deterministyczne sprawdzenia oparte o regu≈Çy (`must_contain_any`, `must_not_contain_any`).

‚úÖ Najlepsze dla:
- kr√≥tkich, jednoznacznych prompt√≥w,
- test√≥w regresji,
- sytuacji gdzie wynik musi zawieraƒá konkretne s≈Çowo/frazƒô.

---

### 2) **Golden tests** (`golden.jsonl`)
Por√≥wnanie outputu modelu do odpowiedzi wzorcowej (token F1).

‚úÖ Najlepsze dla:
- kr√≥tkich/≈õrednich odpowiedzi, kt√≥re mogƒÖ byƒá parafrazƒÖ,
- test√≥w poprawno≈õci tre≈õci bez potrzeby judge.

---

### 3) **Judge tests** (`judge.jsonl`)
LLM-as-a-judge (DeepEval): model ocenia model.

‚úÖ Najlepsze dla:
- z≈Ço≈ºonych scenariuszy,
- wieloetapowego rozumowania,
- odpowiedzi jako≈õciowych/subiektywnych.

‚ö†Ô∏è Wolniejsze, zale≈ºne od jako≈õci modelu judge (musi trzymaƒá format/JSON).

---

## Struktura katalog√≥w

Datasety sƒÖ uporzƒÖdkowane wg **test set√≥w** (kategorii):

```
evals/
  datasets/
    common_sense/
      rules.jsonl
      golden.jsonl
      judge.jsonl
    polish_context/
      rules.jsonl
      golden.jsonl
```

Testy sƒÖ generyczne i automatycznie znajdujƒÖ datasety:

```
evals/tests/
  test_rules_all.py
  test_golden_all.py
  test_judge_all.py
  conftest.py
```

Raporty zapisujƒÖ siƒô do:

```
evals/reports/
```

---

## Wymagania

- Python 3.11+ (rekomendowane)
- Ollama: https://ollama.com/

Testowany model dzia≈Ça lokalnie przez OpenAI-compatible endpoint:
- `http://localhost:11434/v1`

---

## Setup

### 1) Virtualenv

**Windows (PowerShell)**
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -U pip
pip install -r requirements.txt
```

**Linux/macOS**
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -r requirements.txt
```

---

### 2) Konfiguracja `.env`

Utw√≥rz `.env` w root repo:

```env
# Ollama OpenAI-compatible endpoint
OPENAI_BASE_URL=http://localhost:11434/v1
OPENAI_API_KEY=ollama

# Model testowany (SUT)
OLLAMA_MODEL=SpeakLeash/bielik-11b-v3.0-instruct:Q4_K_M

# Model judge (zalecane: ma≈Çy, szybki, dobry w JSON)
OLLAMA_JUDGE_MODEL=phi3:mini

# Timeout dla DeepEval (sekundy)
DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS_OVERRIDE=300
```

---

## Uruchamianie test√≥w (zalecane)

Uruchamiaj przez runner:

```bash
python -m evals.run_tests
```

Tworzy raporty:
- `evals/reports/pytest_report_*.html`
- `evals/reports/pytest_report_*.json`
- `evals/reports/latest_*.html/json`

‚úÖ HTML zawiera logi (prompt + output).

---

## Najczƒôstsze komendy

### Szybkie testy (rules + golden)
```bash
python -m evals.run_tests --fast
```

### Wszystkie testy dla jednego setu (rules + golden + judge)
```bash
python -m evals.run_tests common_sense
```

### Jeden set, szybko (rules + golden)
```bash
python -m evals.run_tests common_sense --fast
```

---

## Uruchamianie konkretnych test set√≥w

### Tylko rules dla danego setu
```bash
python -m evals.run_tests common_sense --rules
```

### Golden + judge dla danego setu
```bash
python -m evals.run_tests common_sense --golden --judge
```

### Kilka set√≥w naraz
```bash
python -m evals.run_tests common_sense extraction --rules
```

---

## Uruchamianie wg typu (dla wszystkich set√≥w)

### Wszystkie rules
```bash
python -m evals.run_tests --rules
```

### Wszystkie golden
```bash
python -m evals.run_tests --golden
```

### Wszystkie judge
```bash
python -m evals.run_tests --judge
```

---

## Wszystkie testy pytest (bez filtr√≥w)

Je≈õli chcesz uruchomiƒá absolutnie wszystko, co pytest wykryje w `evals/tests`:

```bash
python -m evals.run_tests --all-tests
```

To przydaje siƒô, je≈õli w repo pojawiƒÖ siƒô dodatkowe testy poza ‚Äúeval suites‚Äù.

---

## Uruchamianie bez runnera (pytest bezpo≈õrednio)

### Bash (Linux/macOS/Git Bash)
```bash
mkdir -p evals/reports

pytest -vv --capture=tee-sys -rA \
  --json-report --json-report-file=evals/reports/all.json \
  --html=evals/reports/all.html --self-contained-html
```

### PowerShell (Windows)
```powershell
New-Item -ItemType Directory -Force evals\\reports | Out-Null

pytest -vv --capture=tee-sys -rA `
  --json-report --json-report-file=evals\\reports\\all.json `
  --html=evals\\reports\\all.html --self-contained-html
```

---

## Dodawanie nowego test setu (kategorii)

1) Utw√≥rz folder:
```
evals/datasets/<nazwa_setu>/
```

2) Dodaj pliki:
- `rules.jsonl`
- `golden.jsonl`
- `judge.jsonl` (opcjonalnie)

Przyk≈Çady:

**`evals/datasets/common_sense/rules.jsonl`**
```jsonl
{"id":"cs_01","input":"Pada deszcz. Co zabierasz?","must_contain_any":["parasol"]}
```

**`evals/datasets/common_sense/golden.jsonl`**
```jsonl
{"id":"cs_g_01","input":"Po co lod√≥wka?","expected":"Aby d≈Çu≈ºej utrzymaƒá ≈õwie≈ºo≈õƒá jedzenia.","f1_threshold":0.35}
```

**`evals/datasets/common_sense/judge.jsonl`**
```jsonl
{"id":"cs_j_01","input":"Wyja≈õnij, dlaczego pasy w aucie sƒÖ wa≈ºne.","notes":"Should emphasize safety"}
```

3) Odpal:
```bash
python -m evals.run_tests <nazwa_setu>
```

‚úÖ Nie trzeba dodawaƒá nowych plik√≥w `.py` ‚Äî runner sam znajdzie dataset.

---

---

# EN üá¨üáß ‚Äî Evals (rules / golden / judge)

This folder contains evaluation tests for an LLM (large language model) designed to be:

- **easy to run locally** (CPU-only supported),
- **human-reviewable** via HTML reports (prompt + output),
- **CI/CD friendly** via JSON reports,
- **easy to collaborate on** (test cases are stored in `.jsonl`, PR-friendly).

---

## How it works

We support 3 test types:

### 1) **Rules tests** (`rules.jsonl`)
Fast deterministic checks based on rules (`must_contain_any`, `must_not_contain_any`).

‚úÖ Best for:
- short unambiguous prompts,
- regression checks,
- cases where output must contain a specific keyword/phrase.

---

### 2) **Golden tests** (`golden.jsonl`)
Compare model output to a reference answer using similarity scoring (token F1).

‚úÖ Best for:
- short‚Äìmedium answers where paraphrasing is expected,
- correctness checks without needing a judge LLM.

---

### 3) **Judge tests** (`judge.jsonl`)
LLM-as-a-judge (DeepEval): a separate model evaluates the model output.

‚úÖ Best for:
- complex scenarios,
- multi-step reasoning,
- qualitative/subjective answers.

‚ö†Ô∏è Slower, and requires a judge model that follows strict structured/JSON outputs.

---

## Directory structure

Datasets are grouped by **test sets** (categories):

```
evals/
  datasets/
    common_sense/
      rules.jsonl
      golden.jsonl
      judge.jsonl
    polish_context/
      rules.jsonl
      golden.jsonl
```

Generic tests discover datasets automatically:

```
evals/tests/
  test_rules_all.py
  test_golden_all.py
  test_judge_all.py
  conftest.py
```

Reports are saved under:

```
evals/reports/
```

---

## Requirements

- Python 3.11+ (recommended)
- Ollama: https://ollama.com/

We run the model locally through an OpenAI-compatible endpoint:
- `http://localhost:11434/v1`

---

## Setup

### 1) Create a virtual environment

**Windows (PowerShell)**
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -U pip
pip install -r requirements.txt
```

**Linux/macOS**
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -r requirements.txt
```

---

### 2) Configure `.env`

Create `.env` in repo root:

```env
# Ollama OpenAI-compatible endpoint
OPENAI_BASE_URL=http://localhost:11434/v1
OPENAI_API_KEY=ollama

# Model under test (SUT)
OLLAMA_MODEL=SpeakLeash/bielik-11b-v3.0-instruct:Q4_K_M

# Judge model (recommended: small + fast + JSON-friendly)
OLLAMA_JUDGE_MODEL=phi3:mini

# DeepEval timeout (seconds)
DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS_OVERRIDE=300
```

---

## Running tests (recommended)

Use the project runner:

```bash
python -m evals.run_tests
```

It generates:
- `evals/reports/pytest_report_*.html`
- `evals/reports/pytest_report_*.json`
- `evals/reports/latest_*.html/json`

‚úÖ HTML reports include captured logs (prompt + output).

---

## Most common commands

### Fast tests (rules + golden)
```bash
python -m evals.run_tests --fast
```

### All tests for a single set (rules + golden + judge)
```bash
python -m evals.run_tests common_sense
```

### Single set, fast (rules + golden)
```bash
python -m evals.run_tests common_sense --fast
```

---

## Running specific test sets

### Rules only for a test set
```bash
python -m evals.run_tests common_sense --rules
```

### Golden + judge only
```bash
python -m evals.run_tests common_sense --golden --judge
```

### Multiple sets
```bash
python -m evals.run_tests common_sense extraction --rules
```

---

## Running by type across all sets

### Run all rules
```bash
python -m evals.run_tests --rules
```

### Run all golden
```bash
python -m evals.run_tests --golden
```

### Run all judge
```bash
python -m evals.run_tests --judge
```

---

## Run ALL pytest tests (no filtering)

If you want to run *everything* pytest discovers under `evals/tests`:

```bash
python -m evals.run_tests --all-tests
```

Useful if the repository grows additional test files outside the eval suites.

---

## Running without runner (direct pytest)

### Bash (Linux/macOS/Git Bash)
```bash
mkdir -p evals/reports

pytest -vv --capture=tee-sys -rA \
  --json-report --json-report-file=evals/reports/all.json \
  --html=evals/reports/all.html --self-contained-html
```

### PowerShell (Windows)
```powershell
New-Item -ItemType Directory -Force evals\\reports | Out-Null

pytest -vv --capture=tee-sys -rA `
  --json-report --json-report-file=evals\\reports\\all.json `
  --html=evals\\reports\\all.html --self-contained-html
```

---

## Adding a new test set (category)

1) Create a folder:
```
evals/datasets/<set_name>/
```

2) Add dataset files:
- `rules.jsonl`
- `golden.jsonl`
- `judge.jsonl` (optional)

Examples:

**`evals/datasets/common_sense/rules.jsonl`**
```jsonl
{"id":"cs_01","input":"It is raining. What should you take?","must_contain_any":["umbrella"]}
```

**`evals/datasets/common_sense/golden.jsonl`**
```jsonl
{"id":"cs_g_01","input":"Why do people use fridges?","expected":"To keep food fresh longer and slow down spoilage.","f1_threshold":0.35}
```

**`evals/datasets/common_sense/judge.jsonl`**
```jsonl
{"id":"cs_j_01","input":"Explain why seatbelts matter.","notes":"Should emphasize safety"}
```

3) Run:
```bash
python -m evals.run_tests <set_name>
```

‚úÖ No new `.py` files needed ‚Äî tests are discovered automatically.